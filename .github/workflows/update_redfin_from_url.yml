name: Update Redfin raw from URL

on:
  workflow_dispatch:
    inputs:
      level:
        description: "Granularity (city | county | state)"
        required: true
        default: "city"
      raw_url:
        description: "Public URL to the Redfin monthly file (.tsv/.tsv000/.csv)"
        required: true

permissions:
  contents: write

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests gdown

      - name: Download & normalize into data/raw/redfin/<level>/
        env:
          LEVEL: ${{ github.event.inputs.level }}
          RAW_URL: ${{ github.event.inputs.raw_url }}
        run: |
          set -euo pipefail
          python - << 'PY'
          import os, sys, pathlib, re, io, requests, pandas as pd, subprocess, datetime as dt

          level = os.environ["LEVEL"].strip().lower()
          raw_url = os.environ["RAW_URL"].strip()

          if level not in {"city","county","state"}:
            sys.exit(f"Invalid level: {level}")

          out_dir = pathlib.Path("data/raw/redfin")/level
          out_dir.mkdir(parents=True, exist_ok=True)
          stamp = dt.datetime.utcnow().strftime("%Y%m%d")
          dated_path = out_dir/f"monthly_{level}_{stamp}.tsv"
          latest_path = out_dir/"monthly_latest.tsv"

          def is_drive(u:str)->bool:
            return "drive.google.com" in u or "docs.google.com" in u

          # Download bytes
          if is_drive(raw_url):
            # Use gdown for Drive URLs (handles confirm tokens for large files)
            import gdown, tempfile
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tmp.close()
            gdown.download(url=raw_url, output=tmp.name, quiet=False, fuzzy=True)
            data = open(tmp.name, "rb").read()
            os.unlink(tmp.name)
          else:
            r = requests.get(raw_url, timeout=60)
            r.raise_for_status()
            data = r.content

          # Try to infer delimiter: TSV for .tsv/.tsv000; else sniff
          url_lower = raw_url.lower()
          sep = "\t" if (url_lower.endswith(".tsv") or url_lower.endswith(".tsv000")) else None

          # Read to DataFrame
          if sep is None:
            # fallback: try TSV first, then CSV
            try:
              df = pd.read_csv(io.BytesIO(data), sep="\t")
            except Exception:
              df = pd.read_csv(io.BytesIO(data))
          else:
            df = pd.read_csv(io.BytesIO(data), sep=sep)

          # Write canonical TSV (even if input was CSV)
          df.to_csv(dated_path, sep="\t", index=False)
          # Update stable pointer
          df.to_csv(latest_path, sep="\t", index=False)

          print(f"[ok] wrote: {dated_path}")
          print(f"[ok] wrote: {latest_path}")
          PY

      - name: Commit & push
        run: |
          set -e
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/raw/redfin/*
          # Only commit if there are changes
          if ! git diff --cached --quiet; then
            git commit -m "data(redfin): update ${{ github.event.inputs.level }} from URL"
            git push
          else
            echo "No changes to commit."
          fi
