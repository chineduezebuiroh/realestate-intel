name: Update Redfin raw from URLs (multi-level)

on:
  workflow_dispatch:
    inputs:
      city_url:
        description: "Public URL to CITY monthly file (.tsv/.tsv000/.csv)"
        required: false
      county_url:
        description: "Public URL to COUNTY monthly file (.tsv/.tsv000/.csv)"
        required: false
      state_url:
        description: "Public URL to STATE monthly file (.tsv/.tsv000/.csv)"
        required: false

permissions:
  contents: write

concurrency:
  group: update-redfin-raw
  cancel-in-progress: false

jobs:
  slice-and-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyyaml requests gdown


      - name: Slice per-geo from provided URLs (chunked)
        env:
          CITY_URL:   ${{ github.event.inputs.city_url }}
          COUNTY_URL: ${{ github.event.inputs.county_url }}
          STATE_URL:  ${{ github.event.inputs.state_url }}
        run: |
          python - << 'PY'
          import os, io, sys, pathlib, datetime as dt
          import pandas as pd, requests, yaml

          CFG_PATH = "config/markets.yml"
          OUT_ROOT = pathlib.Path("data/raw/redfin")
          CHUNK = 200_000

          def is_drive(url): return "drive.google.com" in url or "docs.google.com" in url
          def fetch_bytes(url: str) -> bytes:
            if not url: return b""
            if is_drive(url):
              import gdown, tempfile
              tmp = tempfile.NamedTemporaryFile(delete=False); tmp.close()
              gdown.download(url=url, output=tmp.name, quiet=False, fuzzy=True)
              b = open(tmp.name, "rb").read(); os.unlink(tmp.name); return b
            r = requests.get(url, timeout=120); r.raise_for_status(); return r.content

          def infer_sep(name: str):
            l = name.lower()
            return "\t" if (l.endswith(".tsv") or l.endswith(".tsv000")) else None

          def read_in_chunks(content: bytes, sep_hint: str | None):
            bio = io.BytesIO(content)
            try:
              yield from pd.read_csv(bio, sep=sep_hint or "\t", chunksize=CHUNK)
              return
            except Exception:
              pass
            bio = io.BytesIO(content)
            yield from pd.read_csv(bio, chunksize=CHUNK)

          # Load markets.yml â†’ by level
          with open(CFG_PATH, "r") as f:
            cfg = yaml.safe_load(f) or {}
          markets = cfg.get("markets", [])
          by_level = {}
          for m in markets:
            lvl = (m.get("level") or m.get("type") or "").lower()
            if not lvl or "geo_id" not in m: continue
            m["match_lc"] = [s.lower() for s in (m.get("match") or [])]
            by_level.setdefault(lvl, []).append(m)

          now = dt.datetime.utcnow().strftime("%Y%m%d")
          url_by_level = {
            "city":   os.environ.get("CITY_URL","").strip(),
            "county": os.environ.get("COUNTY_URL","").strip(),
            "state":  os.environ.get("STATE_URL","").strip(),
          }

          for level, url in url_by_level.items():
            if not url:
              print(f"[{level}] no URL provided; skip"); continue
            if level not in by_level:
              print(f"[{level}] no markets configured; skip"); continue

            data = fetch_bytes(url)
            if not data:
              print(f"[{level}] empty download; skip"); continue

            out_dir = OUT_ROOT / level
            out_dir.mkdir(parents=True, exist_ok=True)

            # open writers lazily when first chunk arrives
            writers = {}  # gid -> {"dated": fh, "latest": fh, "cols": list}
            counts  = {}

            def ensure_writer(gid: str, cols: list[str]):
              if gid in writers: return
              dated  = out_dir / f"{gid}_monthly_{now}.tsv"
              latest = out_dir / f"{gid}_monthly_latest.tsv"
              fh_d = open(dated, "w", encoding="utf-8")
              fh_l = open(latest, "w", encoding="utf-8")
              header = "\t".join(cols) + "\n"
              fh_d.write(header); fh_l.write(header)
              writers[gid] = {"dated": fh_d, "latest": fh_l, "cols": cols}

            # stream
            for chunk in read_in_chunks(data, infer_sep(url)):
              if chunk is None or chunk.empty: continue
              # column lookups
              lc = {c.lower(): c for c in chunk.columns}
              date_col  = lc.get("period_end") or lc.get("period_end_date") or lc.get("month")
              region    = lc.get("region") or lc.get("city") or lc.get("state")
              region_ty = lc.get("region_type") or lc.get("city_type") or lc.get("state_type")
              if not (date_col and region and region_ty):
                print(f"[{level}] missing key cols in chunk; skipping that chunk"); continue

              # normalize date
              chunk = chunk.copy()
              chunk[date_col] = pd.to_datetime(chunk[date_col], errors="coerce").dt.to_period("M").dt.to_timestamp("M")

              # choose columns (keep date + all value cols by default)
              cols = [date_col] + [c for c in chunk.columns if c != date_col]

              for m in by_level[level]:
                gid = m["geo_id"]
                names = m["match_lc"]
                mask = chunk[region_ty].astype(str).str.lower().eq(level) & \
                       chunk[region].astype(str).str.lower().isin(names)
                sub = chunk.loc[mask, cols]
                if sub.empty: continue

                ensure_writer(gid, cols)
                tsv = sub.to_csv(sep="\t", index=False, header=False)
                writers[gid]["dated"].write(tsv)
                writers[gid]["latest"].write(tsv)
                counts[gid] = counts.get(gid, 0) + len(sub)

            # close all writers
            for w in writers.values():
              w["dated"].close(); w["latest"].close()

            for gid, n in sorted(counts.items()):
              print(f"[{level}:{gid}] wrote rows={n}")

          PY

          
       
      - name: Commit & push slim slices
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Stage only if the path exists; don't fail if glob is empty
          if [ -d "data/raw/redfin" ]; then
            git add -A data/raw/redfin || true
          fi

          # Commit only when there are staged changes
          if ! git diff --cached --quiet; then
            git commit -m "data(redfin): update slices from URLs"
            git push
          else
            echo "No changes to commit."
          fi
