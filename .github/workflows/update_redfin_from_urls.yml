name: Update Redfin raw from URLs (multi-level)

on:
  workflow_dispatch:
    inputs:
      city_url:
        description: "Public URL to CITY monthly file (.tsv/.tsv000/.csv)"
        required: false
      county_url:
        description: "Public URL to COUNTY monthly file (.tsv/.tsv000/.csv)"
        required: false
      state_url:
        description: "Public URL to STATE monthly file (.tsv/.tsv000/.csv)"
        required: false

permissions:
  contents: write

concurrency:
  group: update-redfin-raw
  cancel-in-progress: false

jobs:
  slice-and-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyyaml requests gdown


      - name: Slice per-geo from provided URLs (chunked)
        env:
          CITY_URL:   ${{ github.event.inputs.city_url }}
          COUNTY_URL: ${{ github.event.inputs.county_url }}
          STATE_URL:  ${{ github.event.inputs.state_url }}
        run: |
          python - << 'PY'
          import os, io, sys, pathlib, datetime as dt
          import pandas as pd, requests, yaml

          CFG_PATH = "config/markets.yml"
          OUT_ROOT = pathlib.Path("data/raw/redfin")
          CHUNK = 200_000

          def is_drive(url): return "drive.google.com" in url or "docs.google.com" in url
          def fetch_bytes(url: str) -> bytes:
            if not url: return b""
            if is_drive(url):
              import gdown, tempfile
              tmp = tempfile.NamedTemporaryFile(delete=False); tmp.close()
              gdown.download(url=url, output=tmp.name, quiet=False, fuzzy=True)
              b = open(tmp.name, "rb").read(); os.unlink(tmp.name); return b
            r = requests.get(url, timeout=120); r.raise_for_status(); return r.content

          def infer_sep(name: str):
            l = name.lower()
            return "\t" if (l.endswith(".tsv") or l.endswith(".tsv000")) else None

          def read_in_chunks(content: bytes, sep_hint: str | None):
            bio = io.BytesIO(content)
            try:
              yield from pd.read_csv(bio, sep=sep_hint or "\t", chunksize=CHUNK)
              return
            except Exception:
              pass
            bio = io.BytesIO(content)
            yield from pd.read_csv(bio, chunksize=CHUNK)

          import re
          def norm(s:str)->str:
              s = str(s).lower()
              s = re.sub(r"[\.\,']", "", s)  # drop punctuation like .,'
              s = re.sub(r"\s+", " ", s).strip()
              return s

          # Load markets.yml → by level
          with open(CFG_PATH, "r") as f:
            cfg = yaml.safe_load(f) or {}
          markets = cfg.get("markets", [])
          by_level = {}
          for m in markets:
            lvl = (m.get("level") or m.get("type") or "").lower()
            if not lvl or "geo_id" not in m: continue
            m["match_lc"] = [s.lower() for s in (m.get("match") or [])]
            by_level.setdefault(lvl, []).append(m)

          now = dt.datetime.utcnow().strftime("%Y%m%d")
          url_by_level = {
            "city":   os.environ.get("CITY_URL","").strip(),
            "county": os.environ.get("COUNTY_URL","").strip(),
            "state":  os.environ.get("STATE_URL","").strip(),
          }

          for level, url in url_by_level.items():
            if not url:
              print(f"[{level}] no URL provided; skip"); continue
            if level not in by_level:
              print(f"[{level}] no markets configured; skip"); continue

            data = fetch_bytes(url)
            if not data:
              print(f"[{level}] empty download; skip"); continue

            out_dir = OUT_ROOT / level
            out_dir.mkdir(parents=True, exist_ok=True)

            # open writers lazily when first chunk arrives
            writers = {}  # gid -> {"dated": fh, "latest": fh, "cols": list}
            counts  = {}

            def ensure_writer(gid: str, cols: list[str]):
              if gid in writers: return
              dated  = out_dir / f"{gid}_monthly_{now}.tsv"
              latest = out_dir / f"{gid}_monthly_latest.tsv"
              fh_d = open(dated, "w", encoding="utf-8")
              fh_l = open(latest, "w", encoding="utf-8")
              header = "\t".join(cols) + "\n"
              fh_d.write(header); fh_l.write(header)
              writers[gid] = {"dated": fh_d, "latest": fh_l, "cols": cols}

            # stream
            for chunk in read_in_chunks(data, infer_sep(url)):
              if chunk is None or chunk.empty: continue

              # --- tolerant column detection ---
              lc = {c.lower(): c for c in chunk.columns}
              
              # date
              date_col = lc.get("period_end") or lc.get("period_begin") or lc.get("month") or lc.get("last_updated")
              if not date_col:
                  print(f"[{level}] missing date-like column; got:", list(chunk.columns))
                  continue


              chunk = chunk.copy()
              chunk[date_col] = pd.to_datetime(chunk[date_col], errors="coerce").dt.to_period("M").dt.to_timestamp("M")
              
              # location columns (support variants)
              cand_region = ["region","region_name","regionname"]
              cand_city   = ["city"]
              cand_state  = ["state"]
              cand_county = ["county","county_name","countyname"]
              cand_rtype  = ["region_type","regiontype","type","level"]
              
              REGION  = next((lc[x] for x in cand_region if x in lc), None)
              CITY    = next((lc[x] for x in cand_city   if x in lc), None)
              STATE   = next((lc[x] for x in cand_state  if x in lc), None)
              COUNTY  = next((lc[x] for x in cand_county if x in lc), None)
              RTYPE   = next((lc[x] for x in cand_rtype  if x in lc), None)
              
              # normalization
              import re
              def norm(s:str)->str:
                  s = str(s).lower()
                  s = re.sub(r"[\.\,']", "", s)
                  s = re.sub(r"\s+", " ", s).strip()
                  return s
              
              for col in [REGION,CITY,STATE,COUNTY,RTYPE]:
                  if col and col not in chunk:
                      # (shouldn't happen, but safety)
                      continue
              
              chunk["_norm_region"] = chunk[REGION].map(norm) if REGION else ""
              chunk["_norm_city"]   = chunk[CITY].map(norm)   if CITY   else ""
              chunk["_norm_state"]  = chunk[STATE].map(norm)  if STATE  else ""
              chunk["_norm_county"] = chunk[COUNTY].map(norm) if COUNTY else ""
              chunk["_norm_rtype"]  = chunk[RTYPE].astype(str).str.lower() if RTYPE else ""

              # write columns: date + everything else (we’ll subselect later)
              cols = [date_col] + [c for c in chunk.columns if c != date_col]
              
              # --- matching per configured market ---
              for m in by_level[level]:
                  gid = m["geo_id"]
                  names = [norm(x) for x in (m.get("match") or [])]
                  if not names:
                      continue
              
                  # For each level, allow matches on the most relevant columns that exist
                  # and don't hard-require _norm_rtype == level (files differ).
                  series = []
                  if level == "city":
                      if CITY:    series.append(chunk["_norm_city"])
                      if REGION:  series.append(chunk["_norm_region"])
                  elif level == "state":
                      if STATE:   series.append(chunk["_norm_state"])
                      if REGION:  series.append(chunk["_norm_region"])
                  elif level == "county":
                      if COUNTY:  series.append(chunk["_norm_county"])
                      if REGION:  series.append(chunk["_norm_region"])
              
                  import pandas as pd
                  if not series:
                      print(f"[{level}:{gid}] no suitable location columns present; skipping")
                      continue
                  norm_series = series[0]
                  for s in series[1:]:
                      norm_series = norm_series.fillna("") + " | " + s.fillna("")
              
                  # build contains pattern
                  import re
                  pat = "|".join(re.escape(n) for n in names)
                  mask = norm_series.str.contains(pat, na=False)
              
                  sub = chunk.loc[mask, cols]
                  if sub.empty:
                      continue
              
                  ensure_writer(gid, cols)
                  tsv = sub.to_csv(sep="\t", index=False, header=False)
                  writers[gid]["dated"].write(tsv)
                  writers[gid]["latest"].write(tsv)
                  counts[gid] = counts.get(gid, 0) + len(sub)
            
            # close all writers
            for w in writers.values():
              w["dated"].close(); w["latest"].close()

            for gid, n in sorted(counts.items()):
              print(f"[{level}:{gid}] wrote rows={n}")

          PY


      - name: Debug — list produced files & sample
        shell: bash
        run: |
          set -euo pipefail
          echo "Tree under data/raw/redfin:"
          find data/raw/redfin -maxdepth 2 -type f -name '*monthly*.tsv' -print || true
          echo
          for f in data/raw/redfin/*/*_monthly_latest.tsv; do
            echo "---- HEAD $f"
            head -n 3 "$f" || true
            echo
          done


       
      - name: Commit & push slim slices
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Stage only if the path exists; don't fail if glob is empty
          if [ -d "data/raw/redfin" ]; then
            git add -A data/raw/redfin || true
          fi

          # Commit only when there are staged changes
          if ! git diff --cached --quiet; then
            git commit -m "data(redfin): update slices from URLs"
            git push
          else
            echo "No changes to commit."
          fi
