name: Update Redfin raw from URLs (multi-level)

on:
  workflow_dispatch:
    inputs:
      city_url:
        description: "Public URL to CITY monthly file (.tsv/.tsv000/.csv)"
        required: false
      county_url:
        description: "Public URL to COUNTY monthly file (.tsv/.tsv000/.csv)"
        required: false
      state_url:
        description: "Public URL to STATE monthly file (.tsv/.tsv000/.csv)"
        required: false

permissions:
  contents: write

concurrency:
  group: update-redfin-raw
  cancel-in-progress: false

jobs:
  slice-and-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyyaml requests gdown

      - name: Slice per-geo from provided URLs (chunked)
        env:
          CITY_URL:   ${{ github.event.inputs.city_url }}
          COUNTY_URL: ${{ github.event.inputs.county_url }}
          STATE_URL:  ${{ github.event.inputs.state_url }}
        run: |
          python - << 'PY'
          import os, io, sys, re, json, time, pathlib, datetime as dt
          import pandas as pd, requests, yaml
          from typing import Optional, List
          # ---------- config ----------
          CFG_PATH = "config/markets.yml"
          OUT_ROOT = pathlib.Path("data/raw/redfin")
          CHUNK = 200_000  # rows per chunk (adjust if needed)
          KEEP_COLS = None  # None = keep all, or set to mapped subset later
          # ---------- helpers ----------
          def is_drive(url): return "drive.google.com" in url or "docs.google.com" in url
          def fetch_bytes(url: str) -> bytes:
            if not url: return b""
            if is_drive(url):
              import gdown, tempfile
              tmp = tempfile.NamedTemporaryFile(delete=False)
              tmp.close()
              gdown.download(url=url, output=tmp.name, quiet=False, fuzzy=True)
              b = open(tmp.name, "rb").read(); os.unlink(tmp.name); return b
            r = requests.get(url, timeout=120); r.raise_for_status(); return r.content

          def infer_sep(name: str): 
            l = name.lower()
            return "\t" if (l.endswith(".tsv") or l.endswith(".tsv000")) else None

          def read_in_chunks(content: bytes, sep_hint: Optional[str]):
            # try TSV first (Redfin), then CSV
            bio = io.BytesIO(content)
            try:
              yield from pd.read_csv(bio, sep=sep_hint or "\t", chunksize=CHUNK)
              return
            except Exception:
              pass
            bio = io.BytesIO(content)
            yield from pd.read_csv(bio, chunksize=CHUNK)

          # load markets.yml
          with open(CFG_PATH, "r") as f:
            cfg = yaml.safe_load(f)
          markets = cfg.get("markets", [])
          if not markets:
            print("[cfg] no markets defined; nothing to do."); sys.exit(0)

          now = dt.datetime.utcnow().strftime("%Y%m%d")
          url_by_level = {
            "city":   os.environ.get("CITY_URL","").strip(),
            "county": os.environ.get("COUNTY_URL","").strip(),
            "state":  os.environ.get("STATE_URL","").strip(),
          }

          # build matchers per level
          by_level = {}
          for m in markets:
            lvl = m["level"].lower()
            by_level.setdefault(lvl, []).append(m)

          for level, url in url_by_level.items():
            if not url: 
              print(f"[{level}] no URL provided; skip"); 
              continue

            print(f"[{level}] downloadingâ€¦")
            data = fetch_bytes(url)
            if not data:
              print(f"[{level}] empty download; skip"); 
              continue

            OUT_DIR = (OUT_ROOT/level)
            OUT_DIR.mkdir(parents=True, exist_ok=True)

            # prep output writers per geo_id
            writers = {}
            counts  = { }  # geo_id -> rows

            def get_writers(df_cols):
              nonlocal writers
              # choose columns: keep all or a consistent subset
              cols = list(df_cols) if KEEP_COLS is None else [c for c in KEEP_COLS if c in df_cols]
              for m in by_level.get(level, []):
                gid = m["geo_id"]
                dated = OUT_DIR / f"{gid}_monthly_{now}.tsv"
                latest = OUT_DIR / f"{gid}_monthly_latest.tsv"
                if gid not in writers:
                  writers[gid] = {
                    "cols": cols,
                    "dated": open(dated, "w", encoding="utf-8"),
                    "latest": open(latest, "w", encoding="utf-8")
                  }
                  # write headers
                  line = "\t".join(cols) + "\n"
                  writers[gid]["dated"].write(line)
                  writers[gid]["latest"].write(line)

            # streaming filter
            for chunk in read_in_chunks(data, infer_sep(url)):
              # standardize lookup keys
              lc = {c.lower(): c for c in chunk.columns}
              date_col  = lc.get("period_end") or lc.get("period_end_date") or lc.get("month")
              region    = lc.get("region") or lc.get("city") or lc.get("state")
              region_ty = lc.get("region_type") or lc.get("city_type") or lc.get("state_type")
              if not (date_col and region and region_ty):
                print(f"[{level}] missing key columns in chunk; skip this chunk")
                continue

              # init writers once we know columns
              get_writers(chunk.columns)

              # for each configured market at this level, filter & write
              for m in by_level.get(level, []):
                gid = m["geo_id"]
                names = [s.lower() for s in m["match"]]
                mask = chunk[region_ty].astype(str).str.lower().eq(level) & \
                       chunk[region].astype(str).str.lower().isin(names)
                sub = chunk.loc[mask, writers[gid]["cols"]]
                if sub.empty: 
                  continue
                # month-end normalize (optional here; transform will also standardize)
                sub = sub.copy()
                if date_col in sub.columns:
                  sub[date_col] = pd.to_datetime(sub[date_col], errors="coerce").dt.to_period("M").dt.to_timestamp("M")
                # write rows
                tsv = sub.to_csv(sep="\t", index=False, header=False)
                writers[gid]["dated"].write(tsv)
                writers[gid]["latest"].write(tsv)
                counts[gid] = counts.get(gid, 0) + len(sub)

            # close files
            for w in writers.values():
              w["dated"].close(); w["latest"].close()

            for gid, n in sorted(counts.items()):
              print(f"[{level}:{gid}] wrote rows={n}")

          PY

      - name: Commit & push slim slices
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/raw/redfin/**
          if ! git diff --cached --quiet; then
            git commit -m "data(redfin): update slices from URLs"
            git push
          else
            echo "No changes to commit."
