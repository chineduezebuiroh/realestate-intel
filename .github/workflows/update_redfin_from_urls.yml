name: Update Redfin raw from URLs (multi-level)

on:
  workflow_dispatch:
    inputs:
      city_url:
        description: "Public URL to CITY monthly file (.tsv/.tsv000/.csv)"
        required: false
      county_url:
        description: "Public URL to COUNTY monthly file (.tsv/.tsv000/.csv)"
        required: false
      state_url:
        description: "Public URL to STATE monthly file (.tsv/.tsv000/.csv)"
        required: false

permissions:
  contents: write

concurrency:
  group: update-redfin-raw
  cancel-in-progress: false

jobs:
  slice-and-commit:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyyaml requests gdown


      - name: Slice per-geo from provided URLs (chunked)
        env:
          CITY_URL:   ${{ github.event.inputs.city_url }}
          COUNTY_URL: ${{ github.event.inputs.county_url }}
          STATE_URL:  ${{ github.event.inputs.state_url }}
        run: |
          python - << 'PY'
          import os, io, sys, pathlib, datetime as dt
          import pandas as pd, requests, yaml

          CFG_PATH = "config/markets.yml"
          OUT_ROOT = pathlib.Path("data/raw/redfin")
          CHUNK = 200_000

          def is_drive(url): return "drive.google.com" in url or "docs.google.com" in url
          def fetch_bytes(url: str) -> bytes:
            if not url: return b""
            if is_drive(url):
              import gdown, tempfile
              tmp = tempfile.NamedTemporaryFile(delete=False); tmp.close()
              gdown.download(url=url, output=tmp.name, quiet=False, fuzzy=True)
              b = open(tmp.name, "rb").read(); os.unlink(tmp.name); return b
            r = requests.get(url, timeout=120); r.raise_for_status(); return r.content

          def infer_sep(name: str):
            l = name.lower()
            return "\t" if (l.endswith(".tsv") or l.endswith(".tsv000")) else None

          def read_in_chunks(content: bytes, sep_hint: str | None):
            bio = io.BytesIO(content)
            try:
              yield from pd.read_csv(bio, sep=sep_hint or "\t", chunksize=CHUNK)
              return
            except Exception:
              pass
            bio = io.BytesIO(content)
            yield from pd.read_csv(bio, chunksize=CHUNK)

          import re
          def norm(s:str)->str:
              s = str(s).lower()
              s = re.sub(r"[\.\,']", "", s)  # drop punctuation like .,'
              s = re.sub(r"\s+", " ", s).strip()
              return s

          # Load markets.yml â†’ by level
          with open(CFG_PATH, "r") as f:
            cfg = yaml.safe_load(f) or {}
          markets = cfg.get("markets", [])
          by_level = {}
          for m in markets:
            lvl = (m.get("level") or m.get("type") or "").lower()
            if not lvl or "geo_id" not in m: continue
            m["match_lc"] = [s.lower() for s in (m.get("match") or [])]
            by_level.setdefault(lvl, []).append(m)

          now = dt.datetime.utcnow().strftime("%Y%m%d")
          url_by_level = {
            "city":   os.environ.get("CITY_URL","").strip(),
            "county": os.environ.get("COUNTY_URL","").strip(),
            "state":  os.environ.get("STATE_URL","").strip(),
          }

          for level, url in url_by_level.items():
            if not url:
              print(f"[{level}] no URL provided; skip"); continue
            if level not in by_level:
              print(f"[{level}] no markets configured; skip"); continue

            data = fetch_bytes(url)
            if not data:
              print(f"[{level}] empty download; skip"); continue

            out_dir = OUT_ROOT / level
            out_dir.mkdir(parents=True, exist_ok=True)

            # open writers lazily when first chunk arrives
            writers = {}  # gid -> {"dated": fh, "latest": fh, "cols": list}
            counts  = {}

            def ensure_writer(gid: str, cols: list[str]):
              if gid in writers: return
              dated  = out_dir / f"{gid}_monthly_{now}.tsv"
              latest = out_dir / f"{gid}_monthly_latest.tsv"
              fh_d = open(dated, "w", encoding="utf-8")
              fh_l = open(latest, "w", encoding="utf-8")
              header = "\t".join(cols) + "\n"
              fh_d.write(header); fh_l.write(header)
              writers[gid] = {"dated": fh_d, "latest": fh_l, "cols": cols}

            # stream
            for chunk in read_in_chunks(data, infer_sep(url)):
              if chunk is None or chunk.empty: continue

              # --- tolerant column detection (unchanged up to here) ---
              lc = {c.lower(): c for c in chunk.columns}
              
              date_col = lc.get("period_end") or lc.get("period_begin") or lc.get("month") or lc.get("last_updated")
              if not date_col:
                  print(f"[{level}] missing date-like column; got:", list(chunk.columns))
                  continue
              chunk = chunk.copy()
              chunk[date_col] = pd.to_datetime(chunk[date_col], errors="coerce").dt.to_period("M").dt.to_timestamp("M")
              
              REGION     = next((lc.get(x) for x in ["region","region_name","regionname"]), None)
              CITY       = lc.get("city")
              STATE      = lc.get("state")
              COUNTY     = next((lc.get(x) for x in ["county","county_name","countyname"]), None)
              STATE_CODE = lc.get("state_code")
              RTYPE      = next((lc.get(x) for x in ["region_type","regiontype","type","level"]), None)


              import re
              def norm(s:str)->str:
                  s = str(s).lower()
                  s = re.sub(r"[\.\,']", "", s)
                  s = re.sub(r"\s+", " ", s).strip()
                  return s
              
              chunk["_n_region"] = chunk[REGION].map(norm) if REGION else ""
              chunk["_n_city"]   = chunk[CITY].map(norm)   if CITY   else ""
              chunk["_n_state"]  = chunk[STATE].map(norm)  if STATE  else ""
              chunk["_n_county"] = chunk[COUNTY].map(norm) if COUNTY else ""
              chunk["_state_code"]= chunk[STATE_CODE].astype(str).str.upper() if STATE_CODE else ""

              # write columns: date + everything else (weâ€™ll subselect later)
              cols = [date_col] + [c for c in chunk.columns if c != date_col]

              for m in by_level[level]:
                  gid   = m["geo_id"]
                  exact = [norm(x) for x in (m.get("match_exact") or [])]
                  fuzzy = [norm(x) for x in (m.get("match") or [])]
                  sc    = (m.get("state_code") or "").upper()
              
                  # choose the best name column for this level
                  if level == "city":
                      name_series = chunk["_n_city"] if CITY else (chunk["_n_region"] if REGION else None)
                  elif level == "state":
                      name_series = chunk["_n_state"] if STATE else (chunk["_n_region"] if REGION else None)
                  elif level == "county":
                      name_series = chunk["_n_county"] if COUNTY else (chunk["_n_region"] if REGION else None)
                  else:
                      name_series = chunk["_n_region"]
              
                  if name_series is None:
                      print(f"[{level}:{gid}] no suitable name column; skipping")
                      continue
              
                  # optional state_code constraint
                  mask_sc = True
                  if sc and STATE_CODE:
                      mask_sc = (chunk["_state_code"] == sc)
              
                  # exact > fuzzy
                  if exact:
                      mask_name = name_series.isin(exact)
                  elif fuzzy:
                      pat = "|".join(re.escape(n) for n in fuzzy)
                      mask_name = name_series.str.contains(pat, na=False)
                  else:
                      print(f"[{level}:{gid}] no match or match_exact provided; skipping")
                      continue
              
                  mask = mask_sc & mask_name  # ðŸ”¹ no property-type filter
                  sub = chunk.loc[mask, cols]
                  if sub.empty:
                      print(f"[{level}:{gid}] matched 0 rows")
                      continue
              
                  ensure_writer(gid, cols)
                  tsv = sub.to_csv(sep="\t", index=False, header=False)
                  writers[gid]["dated"].write(tsv)
                  writers[gid]["latest"].write(tsv)
                  counts[gid] = counts.get(gid, 0) + len(sub)

            
            # close all writers
            for w in writers.values():
              w["dated"].close(); w["latest"].close()

            for gid, n in sorted(counts.items()):
              print(f"[{level}:{gid}] wrote rows={n}")

          PY


      - name: Debug â€” list produced files & sample
        shell: bash
        run: |
          set -euo pipefail
          echo "Tree under data/raw/redfin:"
          find data/raw/redfin -maxdepth 2 -type f -name '*monthly*.tsv' -print || true
          echo
          for f in data/raw/redfin/*/*_monthly_latest.tsv; do
            echo "---- HEAD $f"
            head -n 3 "$f" || true
            echo
          done


       
      - name: Commit & push slim slices
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Stage only if the path exists; don't fail if glob is empty
          if [ -d "data/raw/redfin" ]; then
            git add -A data/raw/redfin || true
          fi

          # Commit only when there are staged changes
          if ! git diff --cached --quiet; then
            git commit -m "data(redfin): update slices from URLs"
            git push
          else
            echo "No changes to commit."
          fi
