# utils/db.py
import os, duckdb, pathlib, argparse
from dotenv import load_dotenv

load_dotenv()
DUCKDB_PATH = os.getenv("DUCKDB_PATH", "./data/market.duckdb")

DDL = '''
-- Recreate dims with PRIMARY KEYs so we can do idempotent inserts cleanly
CREATE OR REPLACE TABLE dim_market(
  geo_id VARCHAR PRIMARY KEY,
  name   VARCHAR,
  type   VARCHAR,
  fips   VARCHAR
);

CREATE OR REPLACE TABLE dim_metric(
  metric_id  VARCHAR PRIMARY KEY,
  name       VARCHAR,
  frequency  VARCHAR,
  unit       VARCHAR,
  category   VARCHAR
);

CREATE OR REPLACE TABLE dim_source(
  source_id VARCHAR PRIMARY KEY,
  name      VARCHAR,
  url       VARCHAR,
  cadence   VARCHAR,
  license   VARCHAR
);


CREATE TABLE IF NOT EXISTS fact_timeseries(
  geo_id           TEXT NOT NULL,
  metric_id        TEXT NOT NULL,
  date             DATE NOT NULL,
  property_type_id TEXT NOT NULL DEFAULT 'all',
  value            DOUBLE,
  source_id        TEXT,
  PRIMARY KEY (geo_id, metric_id, date, property_type_id)
);



CREATE OR REPLACE TABLE fact_forecast(
  geo_id     VARCHAR,
  metric_id  VARCHAR,
  date       DATE,
  horizon_m  INTEGER,
  forecast   DOUBLE,
  pi_low     DOUBLE,
  pi_high    DOUBLE,
  model_id   VARCHAR,
  backtest_fold INTEGER,
  trained_at TIMESTAMP DEFAULT now()
);

CREATE OR REPLACE TABLE fact_backtest(
  geo_id     VARCHAR,
  metric_id  VARCHAR,
  horizon_m  INTEGER,
  fold       INTEGER,
  mae        DOUBLE,
  mape       DOUBLE,
  rmse       DOUBLE,
  model_id   VARCHAR,
  trained_at TIMESTAMP
);

CREATE OR REPLACE TABLE fact_live_error(
  geo_id     VARCHAR,
  metric_id  VARCHAR,
  date       DATE,
  y_true     DOUBLE,
  y_pred     DOUBLE,
  error      DOUBLE,
  model_id   VARCHAR,
  horizon_m  INTEGER,
  scored_at  TIMESTAMP DEFAULT now()
);

CREATE OR REPLACE TABLE fact_drift(
  geo_id    VARCHAR,
  metric_id VARCHAR,
  feature   VARCHAR,
  psi       DOUBLE,
  test_stat DOUBLE,
  p_value   DOUBLE,
  tested_at TIMESTAMP DEFAULT now()
);

CREATE OR REPLACE TABLE fact_quality(
  geo_id          VARCHAR,
  metric_id       VARCHAR,
  date            DATE,
  completeness    DOUBLE,
  freshness_days  INTEGER,
  anomalies_flag  BOOLEAN,
  checked_at      TIMESTAMP DEFAULT now()
);



#-- ==========================
#-- FORECASTING SCHEMA
#-- ==========================

#-- 1) Metadata for each forecast run (vintage)
CREATE TABLE IF NOT EXISTS forecast_runs (
    run_id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
    created_at TIMESTAMP NOT NULL DEFAULT now(),

    #-- What we’re forecasting
    model_name TEXT NOT NULL,              # e.g. 'sarimax'
    model_version TEXT NOT NULL,           # e.g. 'v1'
    target_metric_id TEXT NOT NULL,        # e.g. 'redfin_median_sale_price'
    target_geo_id TEXT NOT NULL,           # e.g. 'dc_city'
    target_property_type_id INTEGER,       # NULL = geo-level metric w/o property type

    #-- Frequency & horizon
    freq TEXT NOT NULL,                    # e.g. 'M'
    train_start DATE NOT NULL,
    train_end DATE NOT NULL,
    horizon_max_months INTEGER NOT NULL,   # e.g. 12

    #-- Model config / metadata
    algo_params_json JSON,                 # serialized dict of model params
    notes TEXT,
    is_active BOOLEAN NOT NULL DEFAULT TRUE
);

#-- 2) Forecasted values per (run_id, target_date)
CREATE TABLE IF NOT EXISTS forecast_predictions (
    run_id BIGINT NOT NULL,
    target_date DATE NOT NULL,
    horizon_steps INTEGER NOT NULL,        # step index (1..H) in the forecast sequence
    horizon_months INTEGER NOT NULL,       # usually = horizon_steps when monthly
    y_hat DOUBLE NOT NULL,
    y_hat_lo DOUBLE,
    y_hat_hi DOUBLE,

    PRIMARY KEY (run_id, target_date),
    FOREIGN KEY (run_id) REFERENCES forecast_runs(run_id)
);

#-- 3) Evaluation view — wide, per run_id, multi-horizon MAPE
#--    (Assumes monthly data + horizons 1,3,6,12 months)
CREATE OR REPLACE VIEW v_forecast_eval AS
WITH joined AS (
    SELECT
        p.run_id,
        p.target_date,
        p.horizon_months,
        p.y_hat,
        f.value AS actual
    FROM forecast_predictions p
    JOIN forecast_runs r ON r.run_id = p.run_id
    JOIN fact_timeseries f
      ON f.metric_id = r.target_metric_id
     AND f.geo_id = r.target_geo_id
     AND (
         (f.property_type_id IS NULL AND r.target_property_type_id IS NULL)
         OR (f.property_type_id = r.target_property_type_id)
     )
     AND f.date = p.target_date
),
per_horizon AS (
    SELECT
        run_id,
        horizon_months,
        AVG(ABS((actual - y_hat) / NULLIF(actual, 0))) * 100.0 AS mape
    FROM joined
    WHERE actual IS NOT NULL
      AND actual <> 0
    GROUP BY run_id, horizon_months
)
SELECT
    run_id,
    MAX(CASE WHEN horizon_months = 1  THEN mape END) AS mape_1m,
    MAX(CASE WHEN horizon_months = 3  THEN mape END) AS mape_3m,
    MAX(CASE WHEN horizon_months = 6  THEN mape END) AS mape_6m,
    MAX(CASE WHEN horizon_months = 12 THEN mape END) AS mape_12m
FROM per_horizon
GROUP BY run_id;



'''

def build():
    pathlib.Path(os.path.dirname(DUCKDB_PATH) or ".").mkdir(parents=True, exist_ok=True)
    con = duckdb.connect(DUCKDB_PATH)
    con.execute(DDL)
    con.close()
    print(f"[db] Initialized schema at {DUCKDB_PATH}")

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--build", action="store_true")
    args = ap.parse_args()
    if args.build:
        build()
